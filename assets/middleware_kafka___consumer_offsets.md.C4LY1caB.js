import{_ as e,c as a,o as t,aM as s}from"./chunks/framework.Du1sph90.js";const m=JSON.parse('{"title":"__consumer_offsets","description":"","frontmatter":{},"headers":[],"relativePath":"middleware/kafka/__consumer_offsets.md","filePath":"middleware/kafka/__consumer_offsets.md","lastUpdated":1752827238000}'),r={name:"middleware/kafka/__consumer_offsets.md"};function p(c,o,n,l,f,u){return t(),a("div",null,o[0]||(o[0]=[s('<h1 id="consumer-offsets" tabindex="-1">__consumer_offsets <a class="header-anchor" href="#consumer-offsets" aria-label="Permalink to &quot;__consumer_offsets&quot;">​</a></h1><p><strong>将 Consumer 的位移数据作为一条条普通的 Kafka 消息，提交到 consumer<em>offsets 中。可以这么说，consumer</em>offsets 的主要作用是保存 Kafka 消费者的位移信息。</strong></p><p><strong>_<em>consumer</em>offsets也是一个 topic，也有分区。和 kafka 的 topic 基本一致支持自定义写入。但是它是内部的 topic，一般最好不要自动修改。</strong></p><h2 id="消息格式" tabindex="-1">消息格式 <a class="header-anchor" href="#消息格式" aria-label="Permalink to &quot;消息格式&quot;">​</a></h2><ol><li><p><strong>分区消费的 offset</strong></p><p><strong>位移主题的 Key 中应该保存 3 部分内容：</strong></p><p>标识某个消费者组里面某个 topic 的某个分区，已经被消费的位置，即offset。</p></li></ol><blockquote><p>key 是 consumerGroupId+topic+分区号，value就是当前offset的值。</p></blockquote><ol><li><p>用于保存 Consumer Group 信息的消息。</p><p>注册 Consumer Group 时的信息。</p></li><li><p>用于删除 Group 过期位移甚至是删除 Group 的消息。</p></li></ol><blockquote><p>一旦某个 Consumer Group 下的所有 Consumer 实例都停止了，而且它们的位移数据都已被删除时，Kafka 会向位移主题的对应分区写入 tombstone 消息，表明要彻底删除这个 Group 的信息。</p></blockquote><h2 id="创建过程" tabindex="-1">创建过程 <a class="header-anchor" href="#创建过程" aria-label="Permalink to &quot;创建过程&quot;">​</a></h2><p><strong>当 Kafka 集群中的第一个 Consumer 程序启动时，Kafka 会自动创建位移主题。</strong></p><p>创建的位移主题 <code>__consumer_offset</code> 默认 50 个分区，3 个副本。</p><h2 id="更新过程" tabindex="-1">更新过程 <a class="header-anchor" href="#更新过程" aria-label="Permalink to &quot;更新过程&quot;">​</a></h2><p>在 Consumer 端有个参数 <code>enable.auto.commit</code>，来控制是否自动提交 offset。</p><ul><li><p>自动提交</p><p>如果设置为 true，Consumer 端就会自动提交消费数据的位移。提交间隔由 <code>auto.commit.interval.ms</code>来控制。</p></li></ul><blockquote><p>自动提交可能导致重复消费的产生。</p></blockquote><ul><li><p>手动提交</p><p>设置为 false，需要在消费完数据之后手动调用 api 来提交位移，如<code>consumer.commitSync</code>。</p></li></ul><blockquote><p>手动提交可以业务控制提交位移，减少重复消费的产生。</p></blockquote><h2 id="过期消息清除" tabindex="-1">过期消息清除 <a class="header-anchor" href="#过期消息清除" aria-label="Permalink to &quot;过期消息清除&quot;">​</a></h2><p>消费者提交位移的过程，就是向位移主题<code>__consumer_offset</code>发一条消息，消息内容就是当前消费分区的位移。</p><p>如果当前分区没有新消息，这样会导致发的消息内容都是相同的位移。</p><p>其实这时候只需要留最新一条数据即可，原来的数据都需要被清除掉。</p><h3 id="删除策略" tabindex="-1">删除策略 <a class="header-anchor" href="#删除策略" aria-label="Permalink to &quot;删除策略&quot;">​</a></h3><p><code>Compaction</code></p><p>对于同一个 Key 的两条消息 M1 和 M2，如果 M1 的发送时间早于 M2，那么 M1 就是过期消息。Compact 的过程就是<strong>扫描日志的所有消息，剔除那些过期的消息，然后把剩下的消息整理在一起。</strong></p><p><img src="https://tc-cdn.flowus.cn/oss/a3038ac0-ec7d-4645-85ec-2629854056fa/image.png?time=1750932000&amp;token=503e5ffc8d07b70af0f4dfa185c0870a13213b5373aed114c6e4fb6f60fa1451&amp;role=free" alt="image.png" loading="lazy"></p><p><strong>Kafka 提供了专门的后台线程定期地巡检待 Compact 的主题，看看是否存在满足条件的可删除数据</strong>。这个后台线程叫 Log Cleaner。</p><blockquote><p>Log Cleaner 不仅会检查位移主题__Consumer_offset的日志段，业务 topic 同样也会检查。</p></blockquote>',27)]))}const d=e(r,[["render",p]]);export{m as __pageData,d as default};
