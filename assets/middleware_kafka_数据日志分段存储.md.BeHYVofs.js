import{_ as a,c as t,o,aM as l}from"./chunks/framework.Du1sph90.js";const g=JSON.parse('{"title":"数据日志分段存储","description":"","frontmatter":{},"headers":[],"relativePath":"middleware/kafka/数据日志分段存储.md","filePath":"middleware/kafka/数据日志分段存储.md","lastUpdated":1752827238000}'),i={name:"middleware/kafka/数据日志分段存储.md"};function n(r,e,p,d,s,c){return o(),t("div",null,e[0]||(e[0]=[l('<h1 id="数据日志分段存储" tabindex="-1">数据日志分段存储 <a class="header-anchor" href="#数据日志分段存储" aria-label="Permalink to &quot;数据日志分段存储&quot;">​</a></h1><h2 id="数据保存机制" tabindex="-1">数据保存机制 <a class="header-anchor" href="#数据保存机制" aria-label="Permalink to &quot;数据保存机制&quot;">​</a></h2><p><img src="https://s2.loli.net/2025/06/26/4Q9SwYra8dDFPpy.png" alt="image.png" loading="lazy"></p><p>Kafka 的数据是按照分区存储的，以 topic-partition 为目录保存数据。</p><p>数据是存到 log 中，而 log 又引入了LogSegment机制。</p><p><code>log.segment.bytes</code>，默认 1G。当超过1G 之后，日志就会开始分割。</p><p>而日志分段文件以及索引文件都是以基准偏移量（offset）命名的。</p><p>基本每段的日志文件包含一个数据文件和两个索引文件。</p><ul><li>以offset 为索引的 <code>.index</code>。</li><li>以时间戳为索引的 <code>.timeindex</code>。</li></ul><p>索引里面并不是保留全量的数据索引，而是以<strong>稀疏索引</strong>的方式保存（方便使用二分查找快速定位数据）。</p><p><img src="https://s2.loli.net/2025/06/26/nKFA5i8dylUTDOS.png" alt="image.png" loading="lazy"></p><p><img src="https://s2.loli.net/2025/06/26/xLB2qVahErIZfvi.png" alt="image.png" loading="lazy"></p><p>所以也可以看到 Kafka 中分区的数据，是按照顺序写的方式在磁盘上保存，写入比随机写性能要快。</p><h2 id="数据删除机制" tabindex="-1">数据删除机制 <a class="header-anchor" href="#数据删除机制" aria-label="Permalink to &quot;数据删除机制&quot;">​</a></h2><p>Kafka 的数据删除主要受到参数影响。</p><ul><li><code>log.retention.bytes</code>按文件总大小保留</li><li><code>log.retention.hours</code>、<code>log.retention.ms</code> 等按时间保留</li></ul><blockquote><p>Kafka 内部有个 Log Clean 会定时清理过期的日志。</p></blockquote>',17)]))}const m=a(i,[["render",n]]);export{g as __pageData,m as default};
